{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforce \n",
    "\n",
    "This is a tensorflow implementation of reinforce as presented in RL: An Introduction. \n",
    "\n",
    "![Image](images/reinforce_performance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Firstly, we must install the neccesary libraries. \n",
    "\n",
    "- Tensorflow for training. \n",
    "- NumPy for handling matrices\n",
    "- Matplotlib for graphs \n",
    "- Gym for the environment which we are going to interact with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import gym\n",
    "tf.random.set_random_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforce Implementation \n",
    "\n",
    "The init function includes the specification of the model parameters. Which is a network of size (4,24,24,2), where the activation units for the neurons are relu for the hidden layers and softmax for the output. The reason for using softmax as the output is because Reinforce wants to learn a probabilistic mapping of states to actions and softmax allows us to do this. \n",
    "\n",
    "The optimizer that is used is the AdamOptimizer, the reason being that it works. Once I know more about optimizers, I'll add more here. \n",
    "\n",
    "Note the loss that is specified for the loss is the catergorical cross entropy. Might seem weird, but mathematically it works out as the same as the policy gradient theorem when the action we took and want to update is represented as a one hot encoding. \n",
    "\n",
    "The reason for the negative return is because model.fit by default aims to minimize the loss. By changing the return to a negative value it trys to minimise a negative value, which is the same as the maximisation of a positive value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reinforce(object):\n",
    "    #Currently just stolen from DQN\n",
    "    \n",
    "    def __init__(self, state_size, action_size, lr , y ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.actions = np.array(range(self.action_size))\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(24, activation = tf.keras.activations.relu, input_shape = self.state_size),\n",
    "            tf.keras.layers.Dense(24, activation = tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dense(self.action_size, activation = tf.keras.activations.softmax),\n",
    "        ])\n",
    "        \n",
    "        self.model.compile(optimizer = tf.train.AdamOptimizer(lr),\n",
    "                          loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "                          metrics = ['accuracy'])\n",
    "        self.y = y #our discount factor\n",
    "        \n",
    "    def train(self, experience):\n",
    "        return self.model.fit(experience[0],\n",
    "                       experience[1], \n",
    "                       sample_weight = np.squeeze(self.reward_to_return(experience[2])),\n",
    "                       batch_size = experience[0].shape[0],\n",
    "                       verbose = 1)\n",
    "        \n",
    "    def reward_to_return(self, reward):\n",
    "        rtn = np.zeros((reward.shape))\n",
    "        current_rtn = 0\n",
    "        for i in reversed(range(np.max(reward.shape))):       \n",
    "            rtn[i] = reward[i] + (self.y * current_rtn)\n",
    "            current_rtn = rtn[i]\n",
    "        return -rtn #changed back to positive #to minus \n",
    "    \n",
    "    def one_hot(self, actions):\n",
    "        one_hot = np.zeros((np.max(actions.shape),self.action_size))\n",
    "        for num,i in enumerate(actions):\n",
    "            one_hot[num][i] = 1.0\n",
    "        return one_hot     \n",
    "    \n",
    "    def predict(self,state):\n",
    "        return np.random.choice(self.actions, p = self.model.predict(state)[0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "This is for storing the results of an episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBuffer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.first_run = True \n",
    "    \n",
    "    def add_to_buffer(self, e):\n",
    "        \"\"\"\n",
    "        The experience (e) added is in form: \n",
    "        [state, action, reward]\n",
    "        The reason for int(not done) is so that if it's a terminal state\n",
    "        it will be stored as 0 and then will easily allow for calculation\n",
    "        of the state value without the need for any if statements. \n",
    "        \"\"\"\n",
    "        if not self.first_run:\n",
    "            for num, i in enumerate(e):\n",
    "                self.experience[num] = np.vstack([i,self.experience[num]])                \n",
    "        else:\n",
    "            for i in e:\n",
    "                self.experience = [np.array(e[0]),np.array(e[1]),\n",
    "                                   np.array(e[2]),\n",
    "                                  ]\n",
    "\n",
    "            self.first_run = False\n",
    "        \n",
    "    def get_batch(self):\n",
    "        self.first_run = True \n",
    "        return self.experience\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value network\n",
    "When I add in baselines this will be done. The actual algorithm dictates that it should be updated based on the observed return, I know this is problematic for DQN so might create an experience replay buffer for this to aid stability for the learning of the value network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(object):\n",
    "    \n",
    "    def __init__(self, state_size, lr):\n",
    "        # output is 1 because regression\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(24, activation = tf.keras.activations.relu, input_shape = state_size),\n",
    "            tf.keras.layers.Dense(24, activation = tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dense(1, activation = tf.keras.activations.linear)\n",
    "        ])\n",
    "        self.model.compile(optimizer = tf.train.RMSPropOptimizer(lr),\n",
    "                          loss = tf.losses.huber_loss,\n",
    "                          metrics = ['accuracy'])\n",
    "    \n",
    "    def train(self, experience):\n",
    "        return self.model.fit(experience[0],update, verbose = 0 , batch_size = 32)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running_average\n",
    "\n",
    "This is just to remove noise from the observed returns. Makes it easier to see if the algorithm is improving when doing hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_average(data):\n",
    "    new_data =[]\n",
    "    for i in range(len(data)):\n",
    "        new_data.append(np.average(data[max(0, i - 10):i+10]))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the environment. \n",
    "Here is the running of the environment with the hyperparameters that produced the results at the top of the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "policy = Reinforce((4,),2, 0.0005, 0.5)\n",
    "episode_memory = MemoryBuffer()\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "episodes = 4000\n",
    "\n",
    "for _ in range(episodes):\n",
    "    done = False\n",
    "    reward = 0\n",
    "    state = np.array([env.reset()])\n",
    "    actions = [0,0]\n",
    "    while not done: \n",
    "        action = policy.predict(state)\n",
    "        actions[action] += 1\n",
    "        next_state, r, done, _ = env.step(action)\n",
    "        reward += r\n",
    "        episode_memory.add_to_buffer([state,action,r])\n",
    "        state = np.array([next_state])\n",
    "    experience = episode_memory.get_batch()\n",
    "    a = policy.train(experience)\n",
    "    print(reward)\n",
    "    print(actions)\n",
    "    rewards.append(reward)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot(running_average(rewards))\n",
    "plot.ylabel('Averaged reward')\n",
    "plot.xlabel('Episodes')\n",
    "plot.title('Reinforce for CartPole')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
